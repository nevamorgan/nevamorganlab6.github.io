---
title: "ESS 330 - Lab 6: Machine Learning in Hydrology"

author: "Neva Morgan"

date: last-modified

subtitle: "Using Tidymodels & CAMELS Data"
---


```{r}
library(tidyverse)
library(tidymodels)
library(powerjoin)
library(glue)
library(vip)
library(baguette)
library(broom)
```


```{r}

root  <- 'https://gdex.ucar.edu/dataset/camels/file'
```

## **Question 1: Your Turn**

Basics:
```{r}
types <- c("clim", "geol", "soil", "topo", "vege", "hydro")

remote_files  <- glue('{root}/camels_{types}.txt')

local_files   <- glue('data/camels_{types}.txt')

walk2(remote_files, local_files, download.file, quiet = TRUE)

camels <- map(local_files, read_delim, show_col_types = FALSE)

camels <- power_full_join(camels ,by = 'gauge_id')
```


#### **What does it mean for data to be represented under zero_q_freq?**
Q in hydrology is represented as the amount of time it takes for a volume water to flow, discharge; while a zero frequency of discharge looks at the steady flow, that doesn't change over multiple periods of time. Within this data set, the zero_q_fre will refer to the frequency of days where discharge is 0mm per day.

### Exploratory Data Analysis:
```{r}

ggplot(data = camels, aes(x = gauge_lon, y = gauge_lat)) + 
  borders("state", color = "grey50") +
  geom_point(aes(color = q_mean)) +
  scale_color_gradient(low = "pink", high = "dodgerblue") +
  ggthemes::theme_map()
```

## **Question 2: Your Turn**

Objectives:
1. Make 2 maps of the sites, coloring the points by the aridty and p_mean column
2. Add clear labels, titles, and a color scale that makes sense for each parameter.
3. Ensure these render as a single image with your choice of facet_*, patchwork, or ggpubr

1. Model Preparation

```{r}
camels |> 
  select(aridity, p_mean, q_mean) |> 
  drop_na() |> 
  cor()
```

2. Visual EDA:
```{r}
# Create a scatter plot of aridity vs rainfall
ggplot(camels, aes(x = aridity, y = p_mean)) +
  # Add points colored by mean flow
  geom_point(aes(color = q_mean)) +
  # Add a linear regression line
  geom_smooth(method = "lm", color = "red", linetype = 2) +
  # Apply the viridis color scale
  scale_color_viridis_c() +
  # Add a title, axis labels, and theme (w/ legend on the bottom)
  theme_linedraw() + 
  theme(legend.position = "bottom") + 
  labs(title = "Aridity vs Rainfall vs Runnoff", 
       x = "Aridity", 
       y = "Rainfall",
       color = "Mean Flow")
```

Testing a Transformation:
```{r}
ggplot(camels, aes(x = aridity, y = p_mean)) +
  geom_point(aes(color = q_mean)) +
  geom_smooth(method = "lm") +
  scale_color_viridis_c() +
  # Apply log transformations to the x and y axes
  scale_x_log10() + 
  scale_y_log10() +
  theme_linedraw() +
  theme(legend.position = "bottom") + 
  labs(title = "Aridity vs Rainfall vs Runnoff", 
       x = "Aridity", 
       y = "Rainfall",
       color = "Mean Flow")
```

New and Improved:
```{r}
ggplot(camels, aes(x = aridity, y = p_mean)) +
  geom_point(aes(color = q_mean)) +
  geom_smooth(method = "lm") +
  # Apply a log transformation to the color scale
  scale_color_viridis_c(trans = "log") +
  scale_x_log10() + 
  scale_y_log10() +
  theme_linedraw() +
  theme(legend.position = "bottom",
        # Expand the legend width ...
        legend.key.width = unit(2.5, "cm"),
        legend.key.height = unit(.5, "cm")) + 
  labs(title = "Aridity vs Rainfall vs Runnoff", 
       x = "Aridity", 
       y = "Rainfall",
       color = "Mean Flow") 
```
Model Building:

```{r}
#Splitting the data into training and testing

set.seed(123)

camels <- camels |>
  mutate(logQmean = log(q_mean))

camels_split <- initial_split(camels, prop = 0.80)

camels_train <- training(camels_split)

camels_test <- testing(camels_split)

camels_cv <- vfold_cv(camels_train, v = 10)
```


```{r}
#Building the recipe

rec <- recipe(logQmean ~ aridity + p_mean, data = camels_train) %>%
  step_log(all_predictors()) %>%
  step_interact(terms = ~ aridity:p_mean) |>
  step_naomit(all_predictors(), all_outcomes())
```

```{r}
#Calling the data to a linear model

baked_data <- prep(rec, camels_train) |>
  bake(new_data = NULL)

lm_base <- lm(logQmean ~ aridity * p_mean, data = baked_data)

summary(lm_base)
```

```{r}
# Sanity CHECK!

summary(lm(logQmean ~ aridity + p_mean + aridity_x_p_mean, data = baked_data))


```
PASSED!
BUT - now we test to see if our trained model and need to validate the tested data!

```{r}
# It's times to prep, bake, and predict:


test_data <- bake(prep(rec), new_data = camels_test)

test_data$lm_pred <- predict(lm_base, newdata = test_data)
```

Model Evaluation: Statistical and Visual

```{r}
metrics(test_data, truth = logQmean, estimate = lm_pred)
```

```{r}
ggplot(test_data, aes(x = logQmean, y = lm_pred, color = aridity)) +
  scale_color_gradient2(low = "brown", mid = "orange", high = "darkgreen") +
  geom_point() +
  geom_abline(linetype = 2) +
  theme_linedraw() +
  labs(title = "Linear Model: Observed vs. Predicted",
       x = "Observed Log Mean Flow",
       y = "Predicted Log Mean Flow",
       color = "Aridity")
```

#### **Using and workflow Instead**

```{r}
# Defining a model:
lm_model <- linear_reg() %>%
  set_engine('lm') %>%
  set_mode("regression")

#Instantiate the workflow:
lm_wf <- workflow() %>%
  add_recipe(rec) %>%
  add_model(lm_model) %>%
  fit(data = camels_train)

summary(extract_fit_engine(lm_wf))$coefficients

```

Making Predictions

```{r}
#NOW WE USE AUGMENT!

lm_data <- augment(lm_wf, new_data = camels_test)

dim(lm_data)
```

Model Evaluation

```{r}
metrics(lm_data, truth = logQmean, estimate = .pred)
```
```{r}
ggplot(lm_data, aes(x = logQmean, y = .pred, color = aridity)) +
  scale_color_viridis_c() +
  geom_point() +
  geom_abline() +
  theme_linedraw()
```

SWITCHING IT UP!

```{r}
# Using the Random Forest Model!
library(baguette)

rf_model <- rand_forest() %>%
  set_engine("ranger", importance = "impurity") %>%
  set_mode("regression")

rf_wf <- workflow() %>%
  add_recipe(rec) %>%
  add_model(rf_model) %>%
  fit(data = camels_train)
```

```{r}
# Making Predictions:

rf_data <- augment(rf_wf, new_data = camels_test)
dim(rf_data)
```

```{r}
# Model Evaluating:

metrics(rf_data, truth = logQmean, estimate = .pred)
```
```{r}
#Plotting:
ggplot(rf_data, aes(x = logQmean, y = .pred, color = aridity)) +
  scale_color_viridis_c() +
  geom_point() +
  geom_abline() +
  theme_linedraw()
```

**WORKFLOW SET APPROACH!**

```{r}
wf <- workflow_set(list(rec), list(lm_model, rf_model)) %>%
  workflow_map('fit_resamples', resamples = camels_cv)

autoplot(wf)
```

```{r}
rank_results(wf, rank_metric = "rsq", select_best = TRUE)
```


## **Question 3: Building XGBOOST and Neural Networks**

```{r}
# XG Boost Model

b_model <- boost_tree() %>%
  set_engine("xgboost") %>%
  set_mode("regression")

b_wf <- workflow() %>%
  add_recipe(rec) %>%
  add_model(b_model) %>%
  fit(data = camels_train)
```

```{r}
b_data <- augment(b_wf, new_data = camels_test)
dim(b_data)
```

```{r}
metrics(b_data, truth = logQmean, estimate = .pred)
```
```{r}
# Checking metrics:
ggplot(b_data, aes(x = logQmean, y = .pred, colour = aridity)) +
  scale_color_viridis_c() +
  geom_point() +
  geom_abline() +
  theme_linedraw()

```



```{r}
# Neural Network time:

nn_model <- mlp(hidden_units = 5, penalty = 0.01) %>%
  set_engine("nnet") %>%
  set_mode("regression")

nn_wf <- workflow() %>%
  add_recipe(rec) %>%
  add_model(nn_model) %>%
  fit(data = camels_train)

nn_data <- augment(nn_wf, new_data = camels_test)
dim(nn_data)
```
```{r}
metrics(nn_data, truth = logQmean, estimate = .pred)
```
```{r}
ggplot(nn_data, aes(x = logQmean, y = .pred, colour = aridity)) +
  scale_color_viridis_c() +
  geom_point() +
  geom_abline() +
  theme_linedraw()
```

**Combining all (linear regression, random Forest, XG Boost, and Neural Network Models)**

```{r}
wf <- workflow_set(list(rec), list(lm_model, rf_model, b_model, nn_model)) %>%
  workflow_map('fit_resamples', resamples = camels_cv)

autoplot(wf)
```

```{r}
rank_results(wf, rank_metric = "rsq", select_best = TRUE)
```

From these tests, we can understand that the neural network and mlp model are performing better for rmsq and rsq in the linear model than the linear and random forests models.

We will move forward with the neural network model to understand this relationship between Observed Log Mean Flow, Predicted Log Mean Flow, and Aridity.

## **BUILDING MY OWN TEST/TRAIN MODEL**

streamflow predictions

```{r}
# Data Splitting

set.seed(123)

camels <- camels |>
  mutate(logQmean = log(q_mean))

camels_split <- initial_split(camels, prop = 0.75)

camels_train <- training(camels_split)

camels_test <- testing(camels_split)

camels_cv <- vfold_cv(camels_train, v = 10)
```

```{r}
# Recipe

rec <- recipe(logQmean ~ p_mean + max_water_content, data = camels_train) %>%
  step_log(all_predictors()) %>%
  step_interact(terms = ~ p_mean:max_water_content) |>
  step_naomit(all_predictors(), all_outcomes())

```

I chose to look at the relationship the precipitation mean has with the maximum water content of soil profiles. This will help us understand if the discharge mean of streamflow as it is related to the maximum water content that can be retained before becoming discharge. Seen below: 

```{r}

baked_data <- prep(rec, camels_train) |>
  bake(new_data = NULL)

lm_base <- lm(logQmean ~ p_mean * max_water_content, data = baked_data)

summary(lm_base)

```

```{r}
test_data <- bake(prep(rec), new_data = camels_test)

test_data$lm_pred <- predict(lm_base, newdata = test_data)

metrics(test_data, truth = logQmean, estimate = lm_pred)
```

```{r}
# Defining Random Forest Model

library(baguette)

rf_model <- rand_forest() %>%
  set_engine("ranger", importance = "impurity") %>%
  set_mode("regression")


#Creating and Adding it to the workflow set
rf_wf <- workflow() %>%
  add_recipe(rec) %>%
  add_model(rf_model) %>%
  fit(data = camels_train)

```

```{r}
rf_data <- augment(rf_wf, new_data = camels_test)
dim(rf_data)

```

```{r}
metrics(rf_data, truth = logQmean, estimate = .pred)

```

```{r}
ggplot(rf_data, aes(x = logQmean, y = .pred, color = max_water_content)) +
  scale_color_viridis_c() +
  geom_point() +
  geom_abline() +
  theme_linedraw()
```

```{r}
# Defining a Linear Regression Model
lm_model <- linear_reg() %>%
  set_engine('lm') %>%
  set_mode("regression")

#Instantiate the workflow:
lm_wf <- workflow() %>%
  add_recipe(rec) %>%
  add_model(lm_model) %>%
  fit(data = camels_train)

summary(extract_fit_engine(lm_wf))$coefficients
```

```{r}

lm_data <- augment(lm_wf, new_data = camels_test)

dim(lm_data)

```

```{r}
metrics(lm_data, truth = logQmean, estimate = .pred)

ggplot(lm_data, aes(x = logQmean, y = .pred, color = max_water_content)) +
  scale_color_viridis_c() +
  geom_point() +
  geom_abline() +
  theme_linedraw()
```

```{r}
# Defining a XG Boost Model

b_model <- boost_tree() %>%
  set_engine('xgboost') %>%
  set_mode("regression")

b_wf <- workflow() %>%
  add_recipe(rec) %>%
  add_model(b_model) %>%
  fit(data = camels_train)
```

```{r}
b_data <- augment(b_wf, new_data = camels_test)
dim(b_data)

```

```{r}
metrics(b_data, truth = logQmean, estimate = .pred)

```

```{r}
# Checking metrics:
ggplot(b_data, aes(x = logQmean, y = .pred, colour = aridity)) +
  scale_color_viridis_c() +
  geom_point() +
  geom_abline() +
  theme_linedraw()
```

**workflow set and Evaluating**

```{r}
wf_obj <- workflow_set(list(rec), list(rf_model, lm_model, b_model)) %>%
  workflow_map('fit_resamples', resamples = camels_cv)

autoplot(wf_obj)
```

```{r}
rank_results(wf_obj, rank_metric = "rsq", select_best = TRUE)
```

For understanding the relationship max water content of soils and precipitation means as they affect discharge mean rates, we should use the random forest model to continue this testing and training of the camels data. Based on the ranking and autoplot the random forest model performs better for understanding the predicted mean streamflow.

```{r}
# Extract and Evaluate

rf_fit = workflow() |>
  add_recipe(rec) |>
  add_model(rf_model) |>
  fit(camels_train)

a = augment(rf_fit, new_data = camels_train)

ggplot(a, aes(x = .pred, y = logQmean)) +
  geom_point()

vip::vip(rf_fit)
```

```{r}
ggplot(a, aes(x = logQmean, y = .pred, color = max_water_content)) +
  scale_color_gradient2(low = "red", mid = "yellow", high = "blue2") +
  geom_point() +
  geom_abline(linetype = 2) +
  theme_linedraw() +
  labs(title = "Random Forest Model: Observed vs. Predicted Mean Streamflow",
       x = "Observed Log Mean Flow",
       y = "Predicted Log Mean Flow",
       color = "Max Water Content")
```

Based on the graph and evaluated workflow for the predicted mean streamflow as it is related to the observed mean streamflow, we can understand that the max water content of the soil profiles and mean precipitation do have a significant influence on streamflow that is seen in the predicted and observed values. Random forest helped us by being able to handle the large data set, while reducing the overfitting of a decision tree to increase it's accuracy. We can see this in the graph above and the precipiation mean had more importance in relation to mean streamflow discharge. In the beginning of this modelling, we had an R-squared value of 0.7645, understanding that this data was not a successful fit to begin with, responds to how the importance of max water content differs from precipitation mean.




