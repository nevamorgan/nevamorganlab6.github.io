[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ESS 330 - Lab 6: Machine Learning in Hydrology",
    "section": "",
    "text": "This is a Quarto website.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "lab6.html",
    "href": "lab6.html",
    "title": "ESS 330 - Lab 6: Machine Learning in Hydrology",
    "section": "",
    "text": "library(tidyverse)\n\nWarning: package 'tidyverse' was built under R version 4.4.3\n\n\nWarning: package 'ggplot2' was built under R version 4.4.3\n\n\nWarning: package 'tidyr' was built under R version 4.4.3\n\n\nWarning: package 'readr' was built under R version 4.4.3\n\n\nWarning: package 'purrr' was built under R version 4.4.3\n\n\nWarning: package 'dplyr' was built under R version 4.4.3\n\n\nWarning: package 'lubridate' was built under R version 4.4.3\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.4     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(tidymodels)\n\nWarning: package 'tidymodels' was built under R version 4.4.3\n\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.3.0 ──\n✔ broom        1.0.7     ✔ rsample      1.2.1\n✔ dials        1.4.0     ✔ tune         1.3.0\n✔ infer        1.0.7     ✔ workflows    1.2.0\n✔ modeldata    1.4.0     ✔ workflowsets 1.1.0\n✔ parsnip      1.3.1     ✔ yardstick    1.3.2\n✔ recipes      1.1.1     \n\n\nWarning: package 'broom' was built under R version 4.4.3\n\n\nWarning: package 'dials' was built under R version 4.4.3\n\n\nWarning: package 'parsnip' was built under R version 4.4.3\n\n\nWarning: package 'recipes' was built under R version 4.4.3\n\n\nWarning: package 'tune' was built under R version 4.4.3\n\n\nWarning: package 'workflows' was built under R version 4.4.3\n\n\nWarning: package 'yardstick' was built under R version 4.4.3\n\n\n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ scales::discard() masks purrr::discard()\n✖ dplyr::filter()   masks stats::filter()\n✖ recipes::fixed()  masks stringr::fixed()\n✖ dplyr::lag()      masks stats::lag()\n✖ yardstick::spec() masks readr::spec()\n✖ recipes::step()   masks stats::step()\n\nlibrary(powerjoin)\n\nWarning: package 'powerjoin' was built under R version 4.4.3\n\nlibrary(glue)\n\nWarning: package 'glue' was built under R version 4.4.3\n\nlibrary(vip)\n\nWarning: package 'vip' was built under R version 4.4.3\n\n\n\nAttaching package: 'vip'\n\nThe following object is masked from 'package:utils':\n\n    vi\n\nlibrary(baguette)\n\nWarning: package 'baguette' was built under R version 4.4.3\n\nlibrary(broom)\nroot  &lt;- 'https://gdex.ucar.edu/dataset/camels/file'\n\ndownload.file('https://gdex.ucar.edu/dataset/camels/file/camels_attributes_v2.0.pdf', \n              'data/camels_attributes_v2.0.pdf')"
  },
  {
    "objectID": "lab6.html#question-1-your-turn",
    "href": "lab6.html#question-1-your-turn",
    "title": "ESS 330 - Lab 6: Machine Learning in Hydrology",
    "section": "Question 1: Your Turn",
    "text": "Question 1: Your Turn\nBasics:\n\ntypes &lt;- c(\"clim\", \"geol\", \"soil\", \"topo\", \"vege\", \"hydro\")\n\nremote_files  &lt;- glue('{root}/camels_{types}.txt')\n\nlocal_files   &lt;- glue('data/camels_{types}.txt')\n\nwalk2(remote_files, local_files, download.file, quiet = TRUE)\n\ncamels &lt;- map(local_files, read_delim, show_col_types = FALSE)\n\ncamels &lt;- power_full_join(camels ,by = 'gauge_id')\n\n\nWhat does it mean for data to be represented under zero_q_freq?\nQ in hydrology is represented as the amount of time it takes for a volume water to flow, discharge; while a zero frequency of discharge looks at the steady flow, that doesn’t change over multiple periods of time. Within this data set, the zero_q_fre will refer to the frequency of days where discharge is 0mm per day.\n\n\nExploratory Data Analysis:\n\nggplot(data = camels, aes(x = gauge_lon, y = gauge_lat)) + \n  borders(\"state\", color = \"grey50\") +\n  geom_point(aes(color = q_mean)) +\n  scale_color_gradient(low = \"pink\", high = \"dodgerblue\") +\n  ggthemes::theme_map()\n\nWarning: Duplicated aesthetics after name standardisation: colour"
  },
  {
    "objectID": "lab6.html#question-2-your-turn",
    "href": "lab6.html#question-2-your-turn",
    "title": "ESS 330 - Lab 6: Machine Learning in Hydrology",
    "section": "Question 2: Your Turn",
    "text": "Question 2: Your Turn\nObjectives: 1. Make 2 maps of the sites, coloring the points by the aridty and p_mean column 2. Add clear labels, titles, and a color scale that makes sense for each parameter. 3. Ensure these render as a single image with your choice of facet_*, patchwork, or ggpubr\n\nModel Preparation\n\n\ncamels |&gt; \n  select(aridity, p_mean, q_mean) |&gt; \n  drop_na() |&gt; \n  cor()\n\n           aridity     p_mean     q_mean\naridity  1.0000000 -0.7550090 -0.5817771\np_mean  -0.7550090  1.0000000  0.8865757\nq_mean  -0.5817771  0.8865757  1.0000000\n\n\n\nVisual EDA:\n\n\n# Create a scatter plot of aridity vs rainfall\nggplot(camels, aes(x = aridity, y = p_mean)) +\n  # Add points colored by mean flow\n  geom_point(aes(color = q_mean)) +\n  # Add a linear regression line\n  geom_smooth(method = \"lm\", color = \"red\", linetype = 2) +\n  # Apply the viridis color scale\n  scale_color_viridis_c() +\n  # Add a title, axis labels, and theme (w/ legend on the bottom)\n  theme_linedraw() + \n  theme(legend.position = \"bottom\") + \n  labs(title = \"Aridity vs Rainfall vs Runnoff\", \n       x = \"Aridity\", \n       y = \"Rainfall\",\n       color = \"Mean Flow\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nTesting a Transformation:\n\nggplot(camels, aes(x = aridity, y = p_mean)) +\n  geom_point(aes(color = q_mean)) +\n  geom_smooth(method = \"lm\") +\n  scale_color_viridis_c() +\n  # Apply log transformations to the x and y axes\n  scale_x_log10() + \n  scale_y_log10() +\n  theme_linedraw() +\n  theme(legend.position = \"bottom\") + \n  labs(title = \"Aridity vs Rainfall vs Runnoff\", \n       x = \"Aridity\", \n       y = \"Rainfall\",\n       color = \"Mean Flow\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nNew and Improved:\n\nggplot(camels, aes(x = aridity, y = p_mean)) +\n  geom_point(aes(color = q_mean)) +\n  geom_smooth(method = \"lm\") +\n  # Apply a log transformation to the color scale\n  scale_color_viridis_c(trans = \"log\") +\n  scale_x_log10() + \n  scale_y_log10() +\n  theme_linedraw() +\n  theme(legend.position = \"bottom\",\n        # Expand the legend width ...\n        legend.key.width = unit(2.5, \"cm\"),\n        legend.key.height = unit(.5, \"cm\")) + \n  labs(title = \"Aridity vs Rainfall vs Runnoff\", \n       x = \"Aridity\", \n       y = \"Rainfall\",\n       color = \"Mean Flow\") \n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nModel Building:\n\n#Splitting the data into training and testing\n\nset.seed(123)\n\ncamels &lt;- camels |&gt;\n  mutate(logQmean = log(q_mean))\n\ncamels_split &lt;- initial_split(camels, prop = 0.80)\n\ncamels_train &lt;- training(camels_split)\n\ncamels_test &lt;- testing(camels_split)\n\ncamels_cv &lt;- vfold_cv(camels_train, v = 10)\n\n\n#Building the recipe\n\nrec &lt;- recipe(logQmean ~ aridity + p_mean, data = camels_train) %&gt;%\n  step_log(all_predictors()) %&gt;%\n  step_interact(terms = ~ aridity:p_mean) |&gt;\n  step_naomit(all_predictors(), all_outcomes())\n\n\n#Calling the data to a linear model\n\nbaked_data &lt;- prep(rec, camels_train) |&gt;\n  bake(new_data = NULL)\n\nlm_base &lt;- lm(logQmean ~ aridity * p_mean, data = baked_data)\n\nsummary(lm_base)\n\n\nCall:\nlm(formula = logQmean ~ aridity * p_mean, data = baked_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.91162 -0.21601 -0.00716  0.21230  2.85706 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    -1.77586    0.16365 -10.852  &lt; 2e-16 ***\naridity        -0.88397    0.16145  -5.475 6.75e-08 ***\np_mean          1.48438    0.15511   9.570  &lt; 2e-16 ***\naridity:p_mean  0.10484    0.07198   1.457    0.146    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5696 on 531 degrees of freedom\nMultiple R-squared:  0.7697,    Adjusted R-squared:  0.7684 \nF-statistic: 591.6 on 3 and 531 DF,  p-value: &lt; 2.2e-16\n\n\n\n# Sanity CHECK!\n\nsummary(lm(logQmean ~ aridity + p_mean + aridity_x_p_mean, data = baked_data))\n\n\nCall:\nlm(formula = logQmean ~ aridity + p_mean + aridity_x_p_mean, \n    data = baked_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.91162 -0.21601 -0.00716  0.21230  2.85706 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)      -1.77586    0.16365 -10.852  &lt; 2e-16 ***\naridity          -0.88397    0.16145  -5.475 6.75e-08 ***\np_mean            1.48438    0.15511   9.570  &lt; 2e-16 ***\naridity_x_p_mean  0.10484    0.07198   1.457    0.146    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5696 on 531 degrees of freedom\nMultiple R-squared:  0.7697,    Adjusted R-squared:  0.7684 \nF-statistic: 591.6 on 3 and 531 DF,  p-value: &lt; 2.2e-16\n\n\nPASSED! BUT - now we test to see if our trained model and need to validate the tested data!\n\n# It's times to prep, bake, and predict:\n\n\ntest_data &lt;- bake(prep(rec), new_data = camels_test)\n\ntest_data$lm_pred &lt;- predict(lm_base, newdata = test_data)\n\nModel Evaluation: Statistical and Visual\n\nmetrics(test_data, truth = logQmean, estimate = lm_pred)\n\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard       0.583\n2 rsq     standard       0.742\n3 mae     standard       0.390\n\n\n\nggplot(test_data, aes(x = logQmean, y = lm_pred, color = aridity)) +\n  scale_color_gradient2(low = \"brown\", mid = \"orange\", high = \"darkgreen\") +\n  geom_point() +\n  geom_abline(linetype = 2) +\n  theme_linedraw() +\n  labs(title = \"Linear Model: Observed vs. Predicted\",\n       x = \"Observed Log Mean Flow\",\n       y = \"Predicted Log Mean Flow\",\n       color = \"Aridity\")\n\n\n\n\n\n\n\n\n\nUsing and workflow Instead\n\n# Defining a model:\nlm_model &lt;- linear_reg() %&gt;%\n  set_engine('lm') %&gt;%\n  set_mode(\"regression\")\n\n#Instantiate the workflow:\nlm_wf &lt;- workflow() %&gt;%\n  add_recipe(rec) %&gt;%\n  add_model(lm_model) %&gt;%\n  fit(data = camels_train)\n\nsummary(extract_fit_engine(lm_wf))$coefficients\n\n                   Estimate Std. Error    t value     Pr(&gt;|t|)\n(Intercept)      -1.7758557 0.16364755 -10.851710 6.463654e-25\naridity          -0.8839738 0.16144589  -5.475357 6.745512e-08\np_mean            1.4843771 0.15511117   9.569762 4.022500e-20\naridity_x_p_mean  0.1048449 0.07198145   1.456555 1.458304e-01\n\n\nMaking Predictions\n\n#NOW WE USE AUGMENT!\n\nlm_data &lt;- augment(lm_wf, new_data = camels_test)\n\ndim(lm_data)\n\n[1] 135  61\n\n\nModel Evaluation\n\nmetrics(lm_data, truth = logQmean, estimate = .pred)\n\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard       0.583\n2 rsq     standard       0.742\n3 mae     standard       0.390\n\n\n\nggplot(lm_data, aes(x = logQmean, y = .pred, color = aridity)) +\n  scale_color_viridis_c() +\n  geom_point() +\n  geom_abline() +\n  theme_linedraw()\n\n\n\n\n\n\n\n\nSWITCHING IT UP!\n\n# Using the Random Forest Model!\nlibrary(baguette)\n\nrf_model &lt;- rand_forest() %&gt;%\n  set_engine(\"ranger\", importance = \"impurity\") %&gt;%\n  set_mode(\"regression\")\n\nrf_wf &lt;- workflow() %&gt;%\n  add_recipe(rec) %&gt;%\n  add_model(rf_model) %&gt;%\n  fit(data = camels_train)\n\n\n# Making Predictions:\n\nrf_data &lt;- augment(rf_wf, new_data = camels_test)\ndim(rf_data)\n\n[1] 135  60\n\n\n\n# Model Evaluating:\n\nmetrics(rf_data, truth = logQmean, estimate = .pred)\n\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard       0.596\n2 rsq     standard       0.733\n3 mae     standard       0.370\n\n\n\n#Plotting:\nggplot(rf_data, aes(x = logQmean, y = .pred, color = aridity)) +\n  scale_color_viridis_c() +\n  geom_point() +\n  geom_abline() +\n  theme_linedraw()\n\n\n\n\n\n\n\n\nWORKFLOW SET APPROACH!\n\nwf &lt;- workflow_set(list(rec), list(lm_model, rf_model)) %&gt;%\n  workflow_map('fit_resamples', resamples = camels_cv)\n\nWarning: package 'ranger' was built under R version 4.4.3\n\nautoplot(wf)\n\n\n\n\n\n\n\n\n\nrank_results(wf, rank_metric = \"rsq\", select_best = TRUE)\n\n# A tibble: 4 × 9\n  wflow_id          .config .metric  mean std_err     n preprocessor model  rank\n  &lt;chr&gt;             &lt;chr&gt;   &lt;chr&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt; &lt;chr&gt;        &lt;chr&gt; &lt;int&gt;\n1 recipe_rand_fore… Prepro… rmse    0.565  0.0243    10 recipe       rand…     1\n2 recipe_rand_fore… Prepro… rsq     0.770  0.0255    10 recipe       rand…     1\n3 recipe_linear_reg Prepro… rmse    0.569  0.0260    10 recipe       line…     2\n4 recipe_linear_reg Prepro… rsq     0.770  0.0223    10 recipe       line…     2"
  },
  {
    "objectID": "lab6.html#question-3-building-xgboost-and-neural-networks",
    "href": "lab6.html#question-3-building-xgboost-and-neural-networks",
    "title": "ESS 330 - Lab 6: Machine Learning in Hydrology",
    "section": "Question 3: Building XGBOOST and Neural Networks",
    "text": "Question 3: Building XGBOOST and Neural Networks\n\n# XG Boost Model\n\nb_model &lt;- boost_tree() %&gt;%\n  set_engine(\"xgboost\") %&gt;%\n  set_mode(\"regression\")\n\nb_wf &lt;- workflow() %&gt;%\n  add_recipe(rec) %&gt;%\n  add_model(b_model) %&gt;%\n  fit(data = camels_train)\n\n\nb_data &lt;- augment(b_wf, new_data = camels_test)\ndim(b_data)\n\n[1] 135  60\n\n\n\nmetrics(b_data, truth = logQmean, estimate = .pred)\n\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard       0.631\n2 rsq     standard       0.702\n3 mae     standard       0.397\n\n\n\n# Checking metrics:\nggplot(b_data, aes(x = logQmean, y = .pred, colour = aridity)) +\n  scale_color_viridis_c() +\n  geom_point() +\n  geom_abline() +\n  theme_linedraw()\n\n\n\n\n\n\n\n\n\n# Neural Network time:\n\nnn_model &lt;- mlp(hidden_units = 5, penalty = 0.01) %&gt;%\n  set_engine(\"nnet\") %&gt;%\n  set_mode(\"regression\")\n\nnn_wf &lt;- workflow() %&gt;%\n  add_recipe(rec) %&gt;%\n  add_model(nn_model) %&gt;%\n  fit(data = camels_train)\n\nnn_data &lt;- augment(nn_wf, new_data = camels_test)\ndim(nn_data)\n\n[1] 135  61\n\n\n\nmetrics(nn_data, truth = logQmean, estimate = .pred)\n\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard       0.553\n2 rsq     standard       0.767\n3 mae     standard       0.350\n\n\n\nggplot(nn_data, aes(x = logQmean, y = .pred, colour = aridity)) +\n  scale_color_viridis_c() +\n  geom_point() +\n  geom_abline() +\n  theme_linedraw()\n\n\n\n\n\n\n\n\nCombining all (linear regression, random Forest, XG Boost, and Neural Network Models)\n\nwf &lt;- workflow_set(list(rec), list(lm_model, rf_model, b_model, nn_model)) %&gt;%\n  workflow_map('fit_resamples', resamples = camels_cv)\n\nWarning: package 'xgboost' was built under R version 4.4.3\n\nautoplot(wf)\n\n\n\n\n\n\n\n\n\nrank_results(wf, rank_metric = \"rsq\", select_best = TRUE)\n\n# A tibble: 8 × 9\n  wflow_id          .config .metric  mean std_err     n preprocessor model  rank\n  &lt;chr&gt;             &lt;chr&gt;   &lt;chr&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt; &lt;chr&gt;        &lt;chr&gt; &lt;int&gt;\n1 recipe_mlp        Prepro… rmse    0.523  0.0307    10 recipe       mlp       1\n2 recipe_mlp        Prepro… rsq     0.801  0.0257    10 recipe       mlp       1\n3 recipe_rand_fore… Prepro… rmse    0.562  0.0258    10 recipe       rand…     2\n4 recipe_rand_fore… Prepro… rsq     0.771  0.0268    10 recipe       rand…     2\n5 recipe_linear_reg Prepro… rmse    0.569  0.0260    10 recipe       line…     3\n6 recipe_linear_reg Prepro… rsq     0.770  0.0223    10 recipe       line…     3\n7 recipe_boost_tree Prepro… rmse    0.600  0.0289    10 recipe       boos…     4\n8 recipe_boost_tree Prepro… rsq     0.745  0.0268    10 recipe       boos…     4\n\n\nFrom these tests, we can understand that the neural network and mlp model are performing better for rmsq and rsq in the linear model than the linear and random forests models.\nWe will move forward with the neural network model to understand this relationship between Observed Log Mean Flow, Predicted Log Mean Flow, and Aridity."
  },
  {
    "objectID": "lab6.html#building-my-own-testtrain-model",
    "href": "lab6.html#building-my-own-testtrain-model",
    "title": "ESS 330 - Lab 6: Machine Learning in Hydrology",
    "section": "BUILDING MY OWN TEST/TRAIN MODEL",
    "text": "BUILDING MY OWN TEST/TRAIN MODEL\nstreamflow predictions\n\n# Data Splitting\n\nset.seed(123)\n\ncamels &lt;- camels |&gt;\n  mutate(logQmean = log(q_mean))\n\ncamels_split &lt;- initial_split(camels, prop = 0.75)\n\ncamels_train &lt;- training(camels_split)\n\ncamels_test &lt;- testing(camels_split)\n\ncamels_cv &lt;- vfold_cv(camels_train, v = 10)\n\n\n# Recipe\n\nrec &lt;- recipe(logQmean ~ p_mean + max_water_content, data = camels_train) %&gt;%\n  step_log(all_predictors()) %&gt;%\n  step_interact(terms = ~ p_mean:max_water_content) |&gt;\n  step_naomit(all_predictors(), all_outcomes())\n\nI chose to look at the relationship the precipitation mean has with the maximum water content of soil profiles. This will help us understand if the discharge mean of streamflow as it is related to the maximum water content that can be retained before becoming discharge. Seen below:\n\nbaked_data &lt;- prep(rec, camels_train) |&gt;\n  bake(new_data = NULL)\n\nlm_base &lt;- lm(logQmean ~ p_mean * max_water_content, data = baked_data)\n\nsummary(lm_base)\n\n\nCall:\nlm(formula = logQmean ~ p_mean * max_water_content, data = baked_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.70495 -0.31108  0.04349  0.28573  2.06245 \n\nCoefficients:\n                         Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)               -3.2241     0.1451 -22.214  &lt; 2e-16 ***\np_mean                     2.6732     0.1341  19.931  &lt; 2e-16 ***\nmax_water_content         -0.5949     0.1352  -4.400 1.33e-05 ***\np_mean:max_water_content   0.3081     0.1399   2.203   0.0281 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5775 on 498 degrees of freedom\nMultiple R-squared:  0.7668,    Adjusted R-squared:  0.7654 \nF-statistic: 545.9 on 3 and 498 DF,  p-value: &lt; 2.2e-16\n\n\n\ntest_data &lt;- bake(prep(rec), new_data = camels_test)\n\ntest_data$lm_pred &lt;- predict(lm_base, newdata = test_data)\n\nmetrics(test_data, truth = logQmean, estimate = lm_pred)\n\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard       0.572\n2 rsq     standard       0.746\n3 mae     standard       0.427\n\n\n\n# Defining Random Forest Model\n\nlibrary(baguette)\n\nrf_model &lt;- rand_forest() %&gt;%\n  set_engine(\"ranger\", importance = \"impurity\") %&gt;%\n  set_mode(\"regression\")\n\n\n#Creating and Adding it to the workflow set\nrf_wf &lt;- workflow() %&gt;%\n  add_recipe(rec) %&gt;%\n  add_model(rf_model) %&gt;%\n  fit(data = camels_train)\n\n\nrf_data &lt;- augment(rf_wf, new_data = camels_test)\ndim(rf_data)\n\n[1] 168  60\n\n\n\nmetrics(rf_data, truth = logQmean, estimate = .pred)\n\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard       0.600\n2 rsq     standard       0.720\n3 mae     standard       0.399\n\n\n\nggplot(rf_data, aes(x = logQmean, y = .pred, color = max_water_content)) +\n  scale_color_viridis_c() +\n  geom_point() +\n  geom_abline() +\n  theme_linedraw()\n\n\n\n\n\n\n\n\n\n# Defining a Linear Regression Model\nlm_model &lt;- linear_reg() %&gt;%\n  set_engine('lm') %&gt;%\n  set_mode(\"regression\")\n\n#Instantiate the workflow:\nlm_wf &lt;- workflow() %&gt;%\n  add_recipe(rec) %&gt;%\n  add_model(lm_model) %&gt;%\n  fit(data = camels_train)\n\nsummary(extract_fit_engine(lm_wf))$coefficients\n\n                             Estimate Std. Error    t value     Pr(&gt;|t|)\n(Intercept)                -3.2240978  0.1451356 -22.214387 1.734810e-76\np_mean                      2.6732285  0.1341254  19.930812 2.022897e-65\nmax_water_content          -0.5949414  0.1352244  -4.399662 1.326996e-05\np_mean_x_max_water_content  0.3080732  0.1398610   2.202709 2.807261e-02\n\n\n\nlm_data &lt;- augment(lm_wf, new_data = camels_test)\n\ndim(lm_data)\n\n[1] 168  61\n\n\n\nmetrics(lm_data, truth = logQmean, estimate = .pred)\n\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard       0.572\n2 rsq     standard       0.746\n3 mae     standard       0.427\n\nggplot(lm_data, aes(x = logQmean, y = .pred, color = max_water_content)) +\n  scale_color_viridis_c() +\n  geom_point() +\n  geom_abline() +\n  theme_linedraw()\n\n\n\n\n\n\n\n\n\n# Defining a XG Boost Model\n\nb_model &lt;- boost_tree() %&gt;%\n  set_engine('xgboost') %&gt;%\n  set_mode(\"regression\")\n\nb_wf &lt;- workflow() %&gt;%\n  add_recipe(rec) %&gt;%\n  add_model(b_model) %&gt;%\n  fit(data = camels_train)\n\n\nb_data &lt;- augment(b_wf, new_data = camels_test)\ndim(b_data)\n\n[1] 168  60\n\n\n\nmetrics(b_data, truth = logQmean, estimate = .pred)\n\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard       0.671\n2 rsq     standard       0.662\n3 mae     standard       0.425\n\n\n\n# Checking metrics:\nggplot(b_data, aes(x = logQmean, y = .pred, colour = aridity)) +\n  scale_color_viridis_c() +\n  geom_point() +\n  geom_abline() +\n  theme_linedraw()\n\n\n\n\n\n\n\n\nworkflow set and Evaluating\n\nwf_obj &lt;- workflow_set(list(rec), list(rf_model, lm_model, b_model)) %&gt;%\n  workflow_map('fit_resamples', resamples = camels_cv)\n\nautoplot(wf_obj)\n\n\n\n\n\n\n\n\n\nrank_results(wf_obj, rank_metric = \"rsq\", select_best = TRUE)\n\n# A tibble: 6 × 9\n  wflow_id          .config .metric  mean std_err     n preprocessor model  rank\n  &lt;chr&gt;             &lt;chr&gt;   &lt;chr&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt; &lt;chr&gt;        &lt;chr&gt; &lt;int&gt;\n1 recipe_rand_fore… Prepro… rmse    0.566  0.0239    10 recipe       rand…     1\n2 recipe_rand_fore… Prepro… rsq     0.779  0.0191    10 recipe       rand…     1\n3 recipe_linear_reg Prepro… rmse    0.578  0.0248    10 recipe       line…     2\n4 recipe_linear_reg Prepro… rsq     0.766  0.0194    10 recipe       line…     2\n5 recipe_boost_tree Prepro… rmse    0.593  0.0241    10 recipe       boos…     3\n6 recipe_boost_tree Prepro… rsq     0.758  0.0198    10 recipe       boos…     3\n\n\nFor understanding the relationship max water content of soils and precipitation means as they affect discharge mean rates, we should use the random forest model to continue this testing and training of the camels data. Based on the ranking and autoplot the random forest model performs better for understanding the predicted mean streamflow.\n\n# Extract and Evaluate\n\nrf_fit = workflow() |&gt;\n  add_recipe(rec) |&gt;\n  add_model(rf_model) |&gt;\n  fit(camels_train)\n\na = augment(rf_fit, new_data = camels_train)\n\nggplot(a, aes(x = .pred, y = logQmean)) +\n  geom_point()\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\nvip::vip(rf_fit)\n\n\n\n\n\n\n\n\n\nggplot(a, aes(x = logQmean, y = .pred, color = max_water_content)) +\n  scale_color_gradient2(low = \"red\", mid = \"yellow\", high = \"blue2\") +\n  geom_point() +\n  geom_abline(linetype = 2) +\n  theme_linedraw() +\n  labs(title = \"Random Forest Model: Observed vs. Predicted Mean Streamflow\",\n       x = \"Observed Log Mean Flow\",\n       y = \"Predicted Log Mean Flow\",\n       color = \"Max Water Content\")\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nBased on the graph and evaluated workflow for the predicted mean streamflow as it is related to the observed mean streamflow, we can understand that the max water content of the soil profiles and mean precipitation do have a significant influence on streamflow that is seen in the predicted and observed values. Random forest helped us by being able to handle the large data set, while reducing the overfitting of a decision tree to increase it’s accuracy. We can see this in the graph above and the precipiation mean had more importance in relation to mean streamflow discharge. In the beginning of this modelling, we had an R-squared value of 0.7645, understanding that this data was not a successful fit to begin with, responds to how the importance of max water content differs from precipitation mean."
  }
]